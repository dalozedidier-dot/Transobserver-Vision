name: test-data-matrix

on:
  push:
    branches: [main]
  pull_request:
  workflow_dispatch:

jobs:
  discover:
    runs-on: ubuntu-latest
    outputs:
      files_json: ${{ steps.discover.outputs.files_json }}
      files_count: ${{ steps.discover.outputs.files_count }}
      skipped_big: ${{ steps.discover.outputs.skipped_big }}
      limit_mb: ${{ steps.discover.outputs.limit_mb }}
      limit_label: ${{ steps.discover.outputs.limit_label }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Discover CSV files in test_data (light on push, full on manual)
        id: discover
        shell: bash
        run: |
          set -euo pipefail

          python3 - <<'PY'
          import json, os
          from pathlib import Path

          event = os.environ.get("GITHUB_EVENT_NAME", "")
          max_mb = 50 if event == "workflow_dispatch" else 5
          max_bytes = max_mb * 1024 * 1024

          root = Path("test_data")
          files = []
          skipped_big = 0

          if root.exists():
            for p in sorted(root.rglob("*.csv")):
              try:
                if p.stat().st_size <= max_bytes:
                  files.append(p.as_posix())
                else:
                  skipped_big += 1
              except FileNotFoundError:
                continue

          Path("/tmp/files.json").write_text(json.dumps(files, separators=(",", ":")), encoding="utf-8")
          Path("/tmp/meta.json").write_text(json.dumps({
            "event": event,
            "max_mb": max_mb,
            "count": len(files),
            "skipped_big": skipped_big,
          }, separators=(",", ":")), encoding="utf-8")

          summary = []
          summary.append("## Discover")
          summary.append(f"- Event: `{event}`")
          summary.append(f"- Limite: {max_mb} MB")
          summary.append(f"- CSV retenus: {len(files)}")
          summary.append(f"- CSV ignorés (trop gros): {skipped_big}")
          summary.append("")
          summary.append("### Liste")
          for f in files:
            summary.append(f"- `{f}`")
          Path("/tmp/discover_summary.md").write_text("\n".join(summary) + "\n", encoding="utf-8")
          PY

          cat /tmp/discover_summary.md >> "$GITHUB_STEP_SUMMARY"

          echo "files_json=$(cat /tmp/files.json)" >> "$GITHUB_OUTPUT"
          echo "files_count=$(python3 - <<'PY'
          import json
          print(json.load(open("/tmp/meta.json","r",encoding="utf-8"))["count"])
          PY
          )" >> "$GITHUB_OUTPUT"

          echo "skipped_big=$(python3 - <<'PY'
          import json
          print(json.load(open("/tmp/meta.json","r",encoding="utf-8"))["skipped_big"])
          PY
          )" >> "$GITHUB_OUTPUT"

          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "limit_mb=50" >> "$GITHUB_OUTPUT"
            echo "limit_label=50MB" >> "$GITHUB_OUTPUT"
          else
            echo "limit_mb=5" >> "$GITHUB_OUTPUT"
            echo "limit_label=5MB" >> "$GITHUB_OUTPUT"
          fi

  matrix-test:
    needs: discover
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      max-parallel: 5
      matrix:
        file: ${{ fromJson(needs.discover.outputs.files_json) }}
        mode: [mock]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Make safe artifact name
        id: safe
        shell: bash
        run: |
          set -euo pipefail
          SAFE_FILE="${{ matrix.file }}"
          SAFE_FILE="${SAFE_FILE//\//_}"
          SAFE_FILE="${SAFE_FILE// /_}"
          SAFE_FILE="${SAFE_FILE//\(/_}"
          SAFE_FILE="${SAFE_FILE//\)/_}"
          echo "SAFE_FILE=$SAFE_FILE" >> "$GITHUB_ENV"
          echo "safe_file=$SAFE_FILE" >> "$GITHUB_OUTPUT"

      - name: Run smoke_one (log + cycle_id)
        id: smoke
        continue-on-error: true
        shell: bash
        run: |
          set -euo pipefail

          chmod +x tools/run_parallel.sh || true
          chmod +x tools/run_parallel_real.sh || true
          chmod +x tools/smoke_one.py || true

          START_TS="$(date +%s)"

          set +e
          python3 tools/smoke_one.py \
            --file "${{ matrix.file }}" \
            --mode "${{ matrix.mode }}" | tee smoke_log.txt
          RC=$?
          set -e

          END_TS="$(date +%s)"
          DURATION="$((END_TS - START_TS))"

          # Dernière ligne = cycle_id (si le script respecte la convention)
          CYCLE_ID="$(tail -n 1 smoke_log.txt | tr -d '\r')"

          echo "rc=$RC" >> "$GITHUB_OUTPUT"
          echo "duration_s=$DURATION" >> "$GITHUB_OUTPUT"
          echo "cycle_id=$CYCLE_ID" >> "$GITHUB_OUTPUT"

      - name: Build per file stats.json
        if: always()
        shell: bash
        run: |
          set -euo pipefail

          RC="${{ steps.smoke.outputs.rc }}"
          DUR="${{ steps.smoke.outputs.duration_s }}"
          CID="${{ steps.smoke.outputs.cycle_id }}"
          FILE="${{ matrix.file }}"
          MODE="${{ matrix.mode }}"
          EVENT="${{ github.event_name }}"
          LIMIT="${{ needs.discover.outputs.limit_label }}"

          OK="false"
          MANIFEST_PATH="unified_cycles/${CID}/unified_manifest.json"
          if [ "$RC" = "0" ] && [ -f "$MANIFEST_PATH" ]; then
            OK="true"
          fi

          python3 - <<PY
          import json

          ok_str = "${OK}"
          ok = True if ok_str == "true" else False

          data = {
            "event": "${EVENT}",
            "limit_label": "${LIMIT}",
            "mode": "${MODE}",
            "file": "${FILE}",
            "safe_file": "${{ steps.safe.outputs.safe_file }}",
            "rc": int("${RC}" or "1"),
            "ok": ok,
            "duration_s": int("${DUR}" or "0"),
            "cycle_id": "${CID}",
          }

          with open("stats.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
          PY

      - name: Job Summary (per file)
        if: always()
        shell: bash
        run: |
          set -euo pipefail

          echo "## Rapport matrix-test" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- Event: \`${{ github.event_name }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Limite discover: \`${{ needs.discover.outputs.limit_label }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Fichiers retenus: \`${{ needs.discover.outputs.files_count }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Ignorés (trop gros): \`${{ needs.discover.outputs.skipped_big }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- Fichier courant: \`${{ matrix.file }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Mode: \`${{ matrix.mode }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Exit code: \`${{ steps.smoke.outputs.rc }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Durée smoke: \`${{ steps.smoke.outputs.duration_s }} s\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Cycle ID: \`${{ steps.smoke.outputs.cycle_id }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          if [ "${{ steps.smoke.outputs.rc }}" != "0" ]; then
            echo "### Etat" >> "$GITHUB_STEP_SUMMARY"
            echo "- Statut: ECHEC" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "### Etat" >> "$GITHUB_STEP_SUMMARY"
            echo "- Statut: OK" >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Log smoke_one.py (extrait)" >> "$GITHUB_STEP_SUMMARY"
          echo '```text' >> "$GITHUB_STEP_SUMMARY"
          tail -n 80 smoke_log.txt >> "$GITHUB_STEP_SUMMARY" || true
          echo '```' >> "$GITHUB_STEP_SUMMARY"

      - name: Annotation slow run
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          DUR="${{ steps.smoke.outputs.duration_s }}"
          if [ "${DUR}" -ge 15 ]; then
            echo "::warning::Run lent (>=15s) sur ${{ matrix.file }}: ${DUR}s"
          fi

      - name: Upload per file stats
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: stats-${{ matrix.mode }}-${{ needs.discover.outputs.limit_label }}-${{ steps.safe.outputs.safe_file }}
          retention-days: 7
          path: |
            stats.json

      - name: Upload pack (default)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pack-${{ matrix.mode }}-${{ needs.discover.outputs.limit_label }}-${{ steps.safe.outputs.safe_file }}
          retention-days: 7
          path: |
            smoke_log.txt
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/unified_manifest.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/input/fixture.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/input/raw/**

            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/phio/phio_report.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/phio/phio_manifest.json

            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/systemd/extraction_report.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/systemd/ddr_report.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/systemd/e_report.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/systemd/run_manifest.json

            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/sost/dd/dd_report.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/sost/ddr/ddr_report.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/sost/e/e_report.json
            unified_cycles/${{ steps.smoke.outputs.cycle_id }}/sost/run_manifest.json

      - name: Upload full cycle (manual only)
        if: always() && github.event_name == 'workflow_dispatch'
        uses: actions/upload-artifact@v4
        with:
          name: full-${{ matrix.mode }}-${{ needs.discover.outputs.limit_label }}-${{ steps.safe.outputs.safe_file }}
          retention-days: 3
          path: unified_cycles/${{ steps.smoke.outputs.cycle_id }}/

      - name: Fail job if smoke failed
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ "${{ steps.smoke.outputs.rc }}" != "0" ]; then
            exit 1
          fi

  rollup:
    if: always()
    needs: [discover, matrix-test]
    runs-on: ubuntu-latest
    steps:
      - name: Download stats artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: stats-*
          path: _stats
          merge-multiple: true

      - name: Aggregate stats and write summary
        shell: bash
        run: |
          set -euo pipefail

          python3 - <<'PY'
          import json, csv
          from pathlib import Path

          root = Path("_stats")
          stats_files = sorted(root.rglob("stats.json"))

          items = []
          for p in stats_files:
            try:
              items.append(json.loads(p.read_text(encoding="utf-8")))
            except Exception:
              continue

          total = len(items)
          ok = sum(1 for x in items if x.get("ok") is True and int(x.get("rc", 1)) == 0)
          fail = total - ok
          failure_rate = (fail / total * 100.0) if total else 0.0

          durations = [int(x.get("duration_s", 0)) for x in items]
          avg_run = (sum(durations) / len(durations)) if durations else 0.0

          slow_sorted = sorted(items, key=lambda x: int(x.get("duration_s", 0)), reverse=True)
          top_slow = slow_sorted[:10]

          failed_files = [x for x in items if not (x.get("ok") is True and int(x.get("rc", 1)) == 0)]

          out_json = {
            "total": total,
            "ok": ok,
            "fail": fail,
            "failure_rate_percent": round(failure_rate, 2),
            "avg_run_s": round(avg_run, 2),
            "top_slow": [
              {"file": x.get("file"), "duration_s": x.get("duration_s"), "rc": x.get("rc")}
              for x in top_slow
            ],
            "failed": [
              {"file": x.get("file"), "rc": x.get("rc"), "cycle_id": x.get("cycle_id")}
              for x in failed_files
            ],
          }

          Path("stats_aggregate.json").write_text(json.dumps(out_json, ensure_ascii=False, indent=2), encoding="utf-8")

          with open("stats_aggregate.csv", "w", encoding="utf-8", newline="") as f:
            w = csv.DictWriter(f, fieldnames=["file","mode","rc","ok","duration_s","cycle_id","event","limit_label"])
            w.writeheader()
            for x in items:
              w.writerow({
                "file": x.get("file",""),
                "mode": x.get("mode",""),
                "rc": x.get("rc",""),
                "ok": x.get("ok",""),
                "duration_s": x.get("duration_s",""),
                "cycle_id": x.get("cycle_id",""),
                "event": x.get("event",""),
                "limit_label": x.get("limit_label",""),
              })

          lines = []
          lines.append("## Analytics Actions")
          lines.append(f"- CSV retenus: {total}")
          lines.append(f"- OK: {ok}")
          lines.append(f"- Fail: {fail}")
          lines.append(f"- Failure rate: {round(failure_rate, 2)} %")
          lines.append(f"- Avg run time: {round(avg_run, 2)} s")
          lines.append("")
          lines.append("### Top slow (10)")
          for x in top_slow:
            lines.append(f"- `{x.get('file')}`: {x.get('duration_s')} s (rc={x.get('rc')})")
          lines.append("")
          lines.append("### Fichiers en echec")
          if failed_files:
            for x in failed_files[:50]:
              lines.append(f"- `{x.get('file')}` (rc={x.get('rc')}, cycle_id={x.get('cycle_id')})")
          else:
            lines.append("- Aucun")

          Path("rollup_summary.md").write_text("\n".join(lines) + "\n", encoding="utf-8")
          PY

          cat rollup_summary.md >> "$GITHUB_STEP_SUMMARY"

      - name: Upload aggregated stats
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: actions-stats
          retention-days: 14
          path: |
            stats_aggregate.json
            stats_aggregate.csv
